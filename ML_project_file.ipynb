{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LmU8kAEKExYr"
   },
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHEMtc78lWFH"
   },
   "source": [
    "Welcome to the world where fashion meets computer vision! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9QYAKXWrkj-"
   },
   "source": [
    "# **FASHION COLLABORATING WITH COMPUTER VISION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHaCCn-XsEb2"
   },
   "source": [
    "Here, we are importing the basic library functions which are necessary to carry on the operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VqNeI8YkFBRm"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import skimage.io\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ojc7VHysNe1"
   },
   "source": [
    "The data required for this project is in the Google Drive. We are mounting on to COLAB in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4VsRWaFFa8x"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvT9aeLfssIw"
   },
   "source": [
    "In the next step, we are specifying path to few variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_fVxoPvlFWXb"
   },
   "outputs": [],
   "source": [
    "TRAIN_IMAGE_DIR = Path('/content/drive/My Drive/stuff/train')\n",
    "DATA_DIR = Path('/content/drive/My Drive')\n",
    "ROOT_DIR = Path('/content/drive/My Drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqwM8qICs3jN"
   },
   "source": [
    "#**Data Import**\n",
    "Importing labels file through pandas library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l669bBnGFWhi"
   },
   "outputs": [],
   "source": [
    "# import train file \n",
    "import pandas as pd\n",
    "train = pd.read_csv(str(DATA_DIR/'train.csv'))\n",
    "train.head()\n",
    "# train = train[train['ClassId'] <= \"10\"]\n",
    "# train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PS2jwB40FWk6"
   },
   "outputs": [],
   "source": [
    "print(len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZkVcPQUtPYm"
   },
   "source": [
    " extracting image metadata fom json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXuaJhIyFWoB"
   },
   "outputs": [],
   "source": [
    " \n",
    "with open(str(DATA_DIR/'Label.json')) as f:\n",
    "    label_descriptions = json.load(f)\n",
    "\n",
    "label_names = [x['name'] for x in label_descriptions['categories']]\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZe10lnfFWrR"
   },
   "outputs": [],
   "source": [
    "label_df = pd.DataFrame(label_names).reset_index()\n",
    "label_df.columns = ['Id','Labels']\n",
    "label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gq38yvLwFWup"
   },
   "outputs": [],
   "source": [
    "print(len(label_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPD6jSPuGhIs"
   },
   "outputs": [],
   "source": [
    "#ClassId = 0,1 and 10\n",
    "\n",
    "segment_df = train\n",
    "segment_df['CategoryId'] = segment_df['ClassId'].str.split('_').str[0]\n",
    "\n",
    "\n",
    "#df2 = []\n",
    "\"\"\"\n",
    "for i in range(47):\n",
    "  df1 = segment_df[segment_df['CategoryId'] == i]\n",
    "  #print(df1)\n",
    "  df2 = df2.append(df1)\n",
    "  #print(df2)\n",
    "  i += 1\n",
    "  \"\"\"\n",
    "\n",
    "df0 = segment_df[segment_df['CategoryId'] == \"0\"]\n",
    "df1 = segment_df[segment_df['CategoryId'] == \"1\"] \n",
    "df2 = segment_df[segment_df['CategoryId'] == \"2\"]\n",
    "df3 = segment_df[segment_df['CategoryId'] == \"3\"]\n",
    "df4 = segment_df[segment_df['CategoryId'] == \"4\"]\n",
    "df5 = segment_df[segment_df['CategoryId'] == \"5\"]\n",
    "\n",
    "df6 = segment_df[segment_df['CategoryId'] == \"6\"]\n",
    "df7 = segment_df[segment_df['CategoryId'] == \"7\"] \n",
    "df8 = segment_df[segment_df['CategoryId'] == \"8\"]\n",
    "df9 = segment_df[segment_df['CategoryId'] == \"9\"]\n",
    "df10 = segment_df[segment_df['CategoryId'] == \"10\"]\n",
    "df11 = segment_df[segment_df['CategoryId'] == \"11\"]\n",
    "\n",
    "df12 = segment_df[segment_df['CategoryId'] == \"12\"]\n",
    "df13 = segment_df[segment_df['CategoryId'] == \"13\"] \n",
    "df14 = segment_df[segment_df['CategoryId'] == \"14\"]\n",
    "df15 = segment_df[segment_df['CategoryId'] == \"15\"]\n",
    "df16 = segment_df[segment_df['CategoryId'] == \"16\"]\n",
    "df17 = segment_df[segment_df['CategoryId'] == \"17\"]\n",
    "\n",
    "df18 = segment_df[segment_df['CategoryId'] == \"18\"]\n",
    "df19 = segment_df[segment_df['CategoryId'] == \"19\"] \n",
    "df20 = segment_df[segment_df['CategoryId'] == \"20\"]\n",
    "df21 = segment_df[segment_df['CategoryId'] == \"21\"]\n",
    "df22 = segment_df[segment_df['CategoryId'] == \"22\"]\n",
    "df23 = segment_df[segment_df['CategoryId'] == \"23\"]\n",
    "\n",
    "df24 = segment_df[segment_df['CategoryId'] == \"24\"]\n",
    "df25 = segment_df[segment_df['CategoryId'] == \"25\"] \n",
    "df26 = segment_df[segment_df['CategoryId'] == \"26\"]\n",
    "df27 = segment_df[segment_df['CategoryId'] == \"27\"]\n",
    "df28 = segment_df[segment_df['CategoryId'] == \"28\"]\n",
    "df29 = segment_df[segment_df['CategoryId'] == \"29\"]\n",
    "\n",
    "\n",
    "\n",
    "#segment_df = segment_df[:10]\n",
    "df1 = df1.append(df0)\n",
    "df1 = df1.append(df1)\n",
    "df1 = df1.append(df2)\n",
    "df1 = df1.append(df3)\n",
    "df1 = df1.append(df4)\n",
    "df1 = df1.append(df5)\n",
    "\n",
    "df1 = df1.append(df6)\n",
    "df1 = df1.append(df7)\n",
    "df1 = df1.append(df8)\n",
    "df1 = df1.append(df9)\n",
    "df1 = df1.append(df10)\n",
    "df1 = df1.append(df11)\n",
    "\n",
    "df1 = df1.append(df12)\n",
    "df1 = df1.append(df13)\n",
    "df1 = df1.append(df14)\n",
    "df1 = df1.append(df15)\n",
    "df1 = df1.append(df16)\n",
    "df1 = df1.append(df17)\n",
    "\n",
    "df1 = df1.append(df18)\n",
    "df1 = df1.append(df19)\n",
    "df1 = df1.append(df20)\n",
    "df1 = df1.append(df21)\n",
    "df1 = df1.append(df22)\n",
    "df1 = df1.append(df23)\n",
    "\n",
    "df1 = df1.append(df24)\n",
    "df1 = df1.append(df25)\n",
    "df1 = df1.append(df26)\n",
    "df1 = df1.append(df27)\n",
    "df1 = df1.append(df28)\n",
    "df1 = df1.append(df29)\n",
    "\n",
    "\n",
    "\n",
    "segment_df = df1\n",
    "print(\"Total segments: \", len(segment_df))\n",
    "segment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7_N_N_OGhp1"
   },
   "outputs": [],
   "source": [
    "# segment_df = train\n",
    "# segment_df['CategoryId'] = segment_df['ClassId'].str.split('_').str[0]\n",
    "# # segment_df = segment_df[segment_df['ClassId'] <= \"10\"]\n",
    "# # train.head()\n",
    "# print(\"Total segments: \", len(segment_df))\n",
    "# segment_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoPi7dguv1WQ"
   },
   "source": [
    "Rows with the same image are grouped together because the subsequent operations perform at an image level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yp9fVwHTGhtD"
   },
   "outputs": [],
   "source": [
    "\n",
    "image_df = segment_df.groupby('ImageId')['EncodedPixels', 'CategoryId'].agg(lambda x: list(x))\n",
    "size_df = segment_df.groupby('ImageId')['Height', 'Width'].mean()\n",
    "image_df = image_df.join(size_df, on='ImageId')\n",
    "\n",
    "print(\"Total images: \", len(image_df))\n",
    "image_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ATvNSm1JG655"
   },
   "outputs": [],
   "source": [
    "# number of labels per image\n",
    "labels_per_image = image_df['CategoryId'].map(lambda x:len(x)).value_counts().to_frame().reset_index().sort_values(by = 'index')\n",
    "labels_per_image.columns = ['#labels','#images']\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "sns.barplot(labels_per_image['#labels'],labels_per_image['#images'])\n",
    "plt.title(\"Number of Labels per Image\", fontsize=20)\n",
    "plt.xlabel(\"# of labels\", fontsize=20)\n",
    "plt.ylabel(\"# of images\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtNHHVGDG9rW"
   },
   "outputs": [],
   "source": [
    "segment_df['CategoryId'] = segment_df['CategoryId'].astype('int64')\n",
    "labels_per_image2 = segment_df.merge(label_df, how='left', left_on='CategoryId', right_on='Id')\n",
    "labels_per_image3 = labels_per_image2.groupby('Labels')['ImageId'].nunique().to_frame().reset_index()\n",
    "labels_per_image2.head()\n",
    "#labels_per_image3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIby1p7oG-JH"
   },
   "outputs": [],
   "source": [
    "labels_per_image4 = labels_per_image2.groupby('Labels')['ImageId'].count().to_frame().reset_index()\n",
    "labels_per_image4.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZJAKtxYG-MK"
   },
   "outputs": [],
   "source": [
    "labels_per_image4.to_csv('word_cloud_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUAghi6sG-P4"
   },
   "outputs": [],
   "source": [
    "d = {}\n",
    "for i in range(len(labels_per_image4)):\n",
    "    d[labels_per_image4.iloc[i,0]] = labels_per_image4.iloc[i,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQUDzY5pIPQY"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 7))\n",
    "sns.barplot(labels_per_image3['Labels'],labels_per_image3['ImageId'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Labels Distribution in Images\", fontsize=20)\n",
    "plt.xlabel(\"labels\", fontsize=10)\n",
    "plt.ylabel(\"# of images\", fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jo9-r0ABxW4h"
   },
   "source": [
    "As the data is bulk, we limited the data and labels which would help us in obtaintaing faster results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i48sTva2XQqv"
   },
   "outputs": [],
   "source": [
    "count = 1\n",
    "coun = len(os.listdir(TRAIN_IMAGE_DIR))\n",
    "x = []\n",
    "while(count<=5000):\n",
    "  for file in os.listdir(TRAIN_IMAGE_DIR):\n",
    "    if (coun >= 1):\n",
    "      images=file\n",
    "      x.append(images)\n",
    "      count+=1\n",
    "      coun -= 1\n",
    "      #print(coun)\n",
    "print(len(x))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xkDSisSiIPTa"
   },
   "outputs": [],
   "source": [
    "\n",
    "#count = 0\n",
    "#coun = len(os.listdir(TRAIN_IMAGE_DIR))\n",
    "#images = [s for s in os.listdir(TRAIN_IMAGE_DIR) if (i <= 10000) else 0]\n",
    "images = x\n",
    "uploaded_images = pd.DataFrame(images, columns = ['image_name'])\n",
    "image_df = image_df[image_df.index.isin(uploaded_images['image_name'])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ert3W1TbJC7"
   },
   "outputs": [],
   "source": [
    "image_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpprWPqmyEtr"
   },
   "source": [
    "# **Partition data in train and test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qnAJMa8IPWb"
   },
   "outputs": [],
   "source": [
    "\n",
    "FOLD = 0\n",
    "N_FOLDS = 10\n",
    "\n",
    "kf = KFold(n_splits=N_FOLDS, random_state=42, shuffle=True)\n",
    "splits = kf.split(image_df) # ideally, this should be multilabel stratification\n",
    "\n",
    "def get_fold():    \n",
    "    for i, (train_index, valid_index) in enumerate(splits):\n",
    "        if i == FOLD:\n",
    "            return image_df.iloc[train_index], image_df.iloc[valid_index]\n",
    "        \n",
    "train_df, valid_df = get_fold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oAtLyvPlmxI"
   },
   "source": [
    "Apply Mask R-CNN with COCO pretrained weights to the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rHODrYugIPZz"
   },
   "outputs": [],
   "source": [
    "!rm -rf Mask_RCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-HSfVSTyXiC"
   },
   "source": [
    "# **Mask-RCNN implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mNZMxKHIPcp"
   },
   "outputs": [],
   "source": [
    "\n",
    "!git clone https://github.com/Kedar-V/Mask_RCNN.git\n",
    "os.chdir('Mask_RCNN')\n",
    "\n",
    "!rm -rf .git # to prevent an error when the kernel is committed\n",
    "!rm -rf images assets # to prevent displaying images at the bottom of a kernel\n",
    "\n",
    "sys.path.append(ROOT_DIR/'Mask_RCNN')\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDRTqQfQIPit"
   },
   "outputs": [],
   "source": [
    "!wget --quiet https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5\n",
    "!ls -lh mask_rcnn_coco.h5\n",
    "\n",
    "COCO_WEIGHTS_PATH = 'mask_rcnn_coco.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWzbE2WnzD75"
   },
   "source": [
    "#**CONFIGURATIONS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQh6ZXsRy_0y"
   },
   "source": [
    "Mask R-CNN has a load of hyperparameters. I only adjust some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ifhAL1o7Mge1"
   },
   "outputs": [],
   "source": [
    "# Set configuration\n",
    "\n",
    "NUM_CATS = 30  # classification ignoring attributes (only categories)\n",
    "IMAGE_SIZE = 512 # the image size is set to 512, which is the same as the size of submission masks\n",
    "\n",
    "class FashionConfig(Config):\n",
    "    NAME = \"fashion\"\n",
    "    NUM_CLASSES = NUM_CATS + 1 # +1 for the background class\n",
    "    \n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 4 #Batch size - memory error occurs when IMAGES_PER_GPU is too high\n",
    "    #https://datascience.stackexchange.com/questions/29719/how-to-set-batch-size-steps-per-epoch-and-validation-steps\n",
    "\n",
    "    #IMAGE_META_SIZE = 43\n",
    "    \n",
    "    BACKBONE = 'resnet50' #resnet50 will be lighter than resnet101 for training\n",
    "    \n",
    "    IMAGE_MIN_DIM = IMAGE_SIZE\n",
    "    IMAGE_MAX_DIM = IMAGE_SIZE    \n",
    "    IMAGE_RESIZE_MODE = \"none\"\n",
    "    \n",
    "#     RPN_ANCHOR_SCALES = (16, 32, 64, 128, 256)\n",
    "    RPN_ANCHOR_SCALES = (4, 8, 16, 32, 64)\n",
    "    #DETECTION_NMS_THRESHOLD = 0.7\n",
    "\n",
    "    STEPS_PER_EPOCH = 50\n",
    "    VALIDATION_STEPS = 100\n",
    "\n",
    "    MAX_GT_INSTANCES = 2\n",
    "    DETECTION_MAX_INSTANCES = 2\n",
    "    ## balance out losses\n",
    "    # https://stackoverflow.com/questions/55360262/what-exactly-are-the-losses-in-matterport-mask-r-cnn\n",
    "    # https://stackoverflow.com/questions/46272841/what-is-the-loss-function-of-the-mask-rcnn\n",
    "    LOSS_WEIGHTS = {\n",
    "          \"rpn_class_loss\": 1.0, # How well the Region Proposal Network separates background with objetcs\n",
    "          \"rpn_bbox_loss\": 0.8, # How well the RPN localize objects\n",
    "          \"mrcnn_class_loss\": 6.0, # How well the Mask RCNN localize objects\n",
    "          \"mrcnn_bbox_loss\": 6.0, # How well the Mask RCNN recognize each class of object\n",
    "          \"mrcnn_mask_loss\": 6.0 # How well the Mask RCNN segment objects\n",
    "    }\n",
    "    \n",
    "config = FashionConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0KYm_Gmwqa2"
   },
   "source": [
    "Here is the custom function that resizes an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hgEILwuuMg4r"
   },
   "outputs": [],
   "source": [
    "# resizing image to 512X512;\n",
    "def resize_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_AREA)  \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S007o07XMg8E"
   },
   "outputs": [],
   "source": [
    "class FashionDataset(utils.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        super().__init__(self)\n",
    "        \n",
    "        # Add classes\n",
    "#         for i, name in enumerate(label_names):\n",
    "#             print(i, end = \" \")\n",
    "#             print(name)\n",
    "        self.add_class(\"fashion\", 0, \"shirt, blouse\")\n",
    "        self.add_class(\"fashion\", 1, \"top, t-shirt, sweatshirt\")\n",
    "        self.add_class(\"fashion\", 2, \"sweater\")\n",
    "        self.add_class(\"fashion\", 3, \"cardigan\")\n",
    "        self.add_class(\"fashion\", 4, \"jacket\")\n",
    "        \n",
    "        self.add_class(\"fashion\", 5, \"vest\")\n",
    "        self.add_class(\"fashion\", 6, \"pants\")\n",
    "        self.add_class(\"fashion\", 7, \"shorts\")\n",
    "        self.add_class(\"fashion\", 8, \"skirt\")\n",
    "        self.add_class(\"fashion\", 9, \"coat\")\n",
    "        self.add_class(\"fashion\", 10, \"dress\")\n",
    "\n",
    "        self.add_class(\"fashion\", 11, \"jumpsuit\")\n",
    "        self.add_class(\"fashion\", 12, \"cape\")\n",
    "        self.add_class(\"fashion\", 13, \"glasses\")\n",
    "        self.add_class(\"fashion\", 14, \"hat\")\n",
    "        self.add_class(\"fashion\", 15, \"headband, head covering, hair accessory\")\n",
    "        \n",
    "        self.add_class(\"fashion\", 16, \"tie\")\n",
    "        self.add_class(\"fashion\", 17, \"glove\")\n",
    "        self.add_class(\"fashion\", 18, \"watch\")\n",
    "        self.add_class(\"fashion\", 19, \"belt\")\n",
    "        self.add_class(\"fashion\", 20, \"leg warmer\")\n",
    "        self.add_class(\"fashion\", 21, \"tights, stockings\")\n",
    "\n",
    "        \n",
    "        self.add_class(\"fashion\", 22, \"sock\")\n",
    "        self.add_class(\"fashion\", 23, \"shoe\")\n",
    "        self.add_class(\"fashion\", 24, \"bag, wallet\")\n",
    "        self.add_class(\"fashion\", 25, \"scarf\")\n",
    "        self.add_class(\"fashion\", 26, \"umbrella\")\n",
    "        self.add_class(\"fashion\", 27, \"hood\")\n",
    "        self.add_class(\"fashion\", 28, \"collar\")\n",
    "        self.add_class(\"fashion\", 29, \"lapel\")\n",
    "\n",
    "        \n",
    "        # Add images \n",
    "        for i, row in df.iterrows():\n",
    "            self.add_image(\"fashion\", \n",
    "                           image_id=row.name, \n",
    "                           path=str(TRAIN_IMAGE_DIR/row.name), \n",
    "                           labels=row['CategoryId'],\n",
    "                           annotations=row['EncodedPixels'], \n",
    "                           height=row['Height'], width=row['Width'])\n",
    "    def image_reference(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        return info['path'], [label_names[int(x)] for x in info['labels']]\n",
    "    \n",
    "    def load_image(self, image_id):\n",
    "        return resize_image(self.image_info[image_id]['path'])\n",
    "    def load_mask(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "                \n",
    "        mask = np.zeros((IMAGE_SIZE, IMAGE_SIZE, len(info['annotations'])), dtype=np.uint8)\n",
    "        labels = []\n",
    "        \n",
    "        for m, (annotation, label) in enumerate(zip(info['annotations'], info['labels'])):\n",
    "            sub_mask = np.full(info['height']*info['width'], 0, dtype=np.uint8)\n",
    "            annotation = [int(x) for x in annotation.split(' ')]\n",
    "            \n",
    "            for i, start_pixel in enumerate(annotation[::2]):\n",
    "                sub_mask[start_pixel: start_pixel+annotation[2*i+1]] = 1\n",
    "            sub_mask = sub_mask.reshape((info['height'], info['width']), order='F')\n",
    "            sub_mask = cv2.resize(sub_mask, (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "            \n",
    "            mask[:, :, m] = sub_mask\n",
    "            labels.append(int(label)+1)\n",
    "            \n",
    "        return mask, np.array(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dB2Z_4dzcPQ"
   },
   "source": [
    "#**VISUALIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LP9GvNrIMg_c"
   },
   "outputs": [],
   "source": [
    "# Visualizing random images\n",
    "dataset = FashionDataset(image_df)\n",
    "dataset.prepare()\n",
    "\n",
    "for i in range(1):\n",
    "    image_id = random.choice(dataset.image_ids)\n",
    "    print(dataset.image_reference(image_id))\n",
    "    \n",
    "    image = dataset.load_image(image_id)\n",
    "    mask, class_ids = dataset.load_mask(image_id)\n",
    "    print(mask)\n",
    "    plt.imshow(image)\n",
    "    #visualize.display_top_masks(image, mask, class_ids, dataset.class_names, limit=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blGrhJNIznzu"
   },
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VgY48EnpMhCi"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = FashionDataset(train_df)\n",
    "train_dataset.prepare()\n",
    "\n",
    "valid_dataset = FashionDataset(valid_df)\n",
    "valid_dataset.prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFaS2upszswo"
   },
   "source": [
    "# Image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfgKds6bMhIL"
   },
   "outputs": [],
   "source": [
    "\n",
    "augmentation = iaa.Sequential([\n",
    "    iaa.OneOf([ ## rotate\n",
    "        iaa.Affine(rotate=0),\n",
    "        iaa.Affine(rotate=90),\n",
    "        iaa.Affine(rotate=180),\n",
    "        iaa.Affine(rotate=270),\n",
    "    ]),\n",
    "    iaa.Fliplr(0.5),\n",
    "    iaa.Flipud(0.5),\n",
    "    iaa.OneOf([ ## brightness or contrast\n",
    "        iaa.Multiply((0.9, 1.1)),\n",
    "        iaa.ContrastNormalization((0.9, 1.1)),\n",
    "    ]),\n",
    "    iaa.OneOf([ ## blur or sharpen\n",
    "        iaa.GaussianBlur(sigma=(0.0, 0.3)),\n",
    "        iaa.Sharpen(alpha=(0.0, 0.3)),\n",
    "    ]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4FWP01QzxMf"
   },
   "source": [
    "# sample output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yMetUL0-MhOS"
   },
   "outputs": [],
   "source": [
    "\n",
    "imggrid = augmentation.draw_grid(image, cols=5, rows=2)\n",
    "plt.figure(figsize=(20, 10))\n",
    "_ = plt.imshow(imggrid.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gcjXgW9z2ka"
   },
   "source": [
    "# initiating Mask R-CNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1t4r9SZpZNd"
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FlghKUKq2_3N"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==1.14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXF1tGwslmbt"
   },
   "outputs": [],
   "source": [
    "!pip install keras==2.2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EBUC6s0yNYHE"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = modellib.MaskRCNN(mode='training', config=config, model_dir=ROOT_DIR);\n",
    "model.load_weights(COCO_WEIGHTS_PATH, by_name=True, exclude=[\n",
    "    'mrcnn_class_logits', 'mrcnn_bbox_fc', 'mrcnn_bbox', 'mrcnn_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ZW751LhQcPO"
   },
   "outputs": [],
   "source": [
    "# Declaring learning rate\n",
    "LR = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_1Hn_EMRsyM"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model.train(train_dataset, valid_dataset,\n",
    "            learning_rate=LR/6,\n",
    "            epochs=10,\n",
    "            layers='all',\n",
    "            augmentation=augmentation)\n",
    "\n",
    "history = model.keras_model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wM5HdNmKRwC-"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model.train(train_dataset, valid_dataset,\n",
    "            learning_rate=LR/8,\n",
    "            epochs=15,\n",
    "            layers='all',\n",
    "            augmentation=augmentation)\n",
    "\n",
    "new_history = model.keras_model.history.history\n",
    "for k in new_history: history[k] = history[k] + new_history[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YTKX6nvPI37-"
   },
   "outputs": [],
   "source": [
    "epochs = range(1, len(history['loss'])+1)\n",
    "pd.DataFrame(history, index=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nMkIyWZ3C-on"
   },
   "outputs": [],
   "source": [
    "# find best epoch\n",
    "best_epoch = np.argmin(history[\"val_loss\"]) + 1\n",
    "print(\"Best epoch: \", best_epoch)\n",
    "print(\"Valid loss: \", history[\"val_loss\"][best_epoch-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9s9OYBG5DWFl"
   },
   "outputs": [],
   "source": [
    "#glob_list = glob.glob(f'/content/drive/My Drive/fashion20201208T0049/mask_rcnn_fashion_0001.h5')\n",
    "#model_path = glob_list[0] if glob_list else ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCUqC72v0E3q"
   },
   "source": [
    "# Prediction, this cell defines InferenceConfig and loads the best trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfrFnwNlEOkx"
   },
   "outputs": [],
   "source": [
    "\n",
    "class InferenceConfig(FashionConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "model = modellib.MaskRCNN(mode='inference', \n",
    "                          config=inference_config,\n",
    "                          model_dir=ROOT_DIR)\n",
    "\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpb1AzCY0IuA"
   },
   "source": [
    "we have to fix the overlappsed masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YamOcAoqFlc5"
   },
   "outputs": [],
   "source": [
    "\n",
    "def refine_masks(masks, rois):\n",
    "    areas = np.sum(masks.reshape(-1, masks.shape[-1]), axis=0)\n",
    "    mask_index = np.argsort(areas)\n",
    "    union_mask = np.zeros(masks.shape[:-1], dtype=bool)\n",
    "    for m in mask_index:\n",
    "        masks[:, :, m] = np.logical_and(masks[:, :, m], np.logical_not(union_mask))\n",
    "        union_mask = np.logical_or(masks[:, :, m], union_mask)\n",
    "    for m in range(masks.shape[-1]):\n",
    "        mask_pos = np.where(masks[:, :, m]==True)\n",
    "        if np.any(mask_pos):\n",
    "            y1, x1 = np.min(mask_pos, axis=1)\n",
    "            y2, x2 = np.max(mask_pos, axis=1)\n",
    "            rois[m, :] = [y1, x1, y2, x2]\n",
    "    return masks, rois"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sB4Fum7q0VnM"
   },
   "source": [
    " Let’s load an image and try to see how the model performs. You can use any of your images to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTyxWPc2F0HM"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load a random image from the images folder\n",
    "import skimage.io\n",
    "image_path = '/content/drive/My Drive/f9c5f3efdae7ce33986891e0ba2f0a9c.jpg'\n",
    "\n",
    "# original image\n",
    "plt.figure(figsize=(12,10))\n",
    "skimage.io.imshow(skimage.io.imread(image_path))\n",
    "\n",
    "img = skimage.io.imread(image_path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "result = model.detect([resize_image(image_path)])\n",
    "r = result[0]\n",
    "if r['masks'].size > 0:\n",
    "    masks = np.zeros((img.shape[0], img.shape[1], r['masks'].shape[-1]), dtype=np.uint8)\n",
    "    for m in range(r['masks'].shape[-1]):\n",
    "        masks[:, :, m] = cv2.resize(r['masks'][:, :, m].astype('uint8'), \n",
    "                                    (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    y_scale = img.shape[0]/IMAGE_SIZE\n",
    "    x_scale = img.shape[1]/IMAGE_SIZE\n",
    "    rois = (r['rois'] * [y_scale, x_scale, y_scale, x_scale]).astype(int)\n",
    "    \n",
    "    masks, rois = refine_masks(masks, rois)\n",
    "else:\n",
    "    masks, rois = r['masks'], r['rois']\n",
    "\n",
    "visualize.display_instances(img, rois, masks, r['class_ids'], \n",
    "                            ['bg']+label_names, r['scores'])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Code_file.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
